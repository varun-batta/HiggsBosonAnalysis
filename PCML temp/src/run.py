# -*- coding: utf-8 -*-
"""
Main script.
"""

import numpy as np
import pickle
from matplotlib import pyplot as plt
import clean_data
import cross_validation
import gradient_descent
import helpers
import polynomial_expansion
import proj1_helpers

################################################################################
#                                    constants                                 #
################################################################################
# data file
DATA_TRAIN_PATH = '../data/train.csv'
DATA_TEST_PATH  = '../data/test.csv'
DATA_PREDICTIONS_PATH='../data/predictions.csv'
invalid_field_value = -999

# polynomial expansion
degree = 2

# randomization seed
seed = 2

# optimization parameters
gd_func = gradient_descent.logistic_L2_gradient_descent
max_iters = 7000
gamma = 0.08
lamb = 0.0
initial_w_range = (-0.001, 0.001) # interval for initial_w parameters

################################################################################
#                                    read data                                 #
################################################################################
(y, X, id) = proj1_helpers.load_csv_data(DATA_TRAIN_PATH, sub_sample=False)
(yTest, XTest, idTest) = proj1_helpers.load_csv_data(DATA_TEST_PATH, sub_sample=False)

# y is categorical, so we want integers (-1, 1) instead of floats (-1.0, 1.0)
# Modified here instead of in load_csv_data, because we don't know if we have the
# right to change the provided functions.
y = y.astype(int)

# The formulas used for the cost and gradients of the logistic function expect
# categories that are 0/1 for some terms to disappear in the equations.
y[np.where(y == -1)] = 0

################################################################################
#                                   clean data                                 #
################################################################################
# Clean the train and test data - remove -999 outliers.
X, XTest = clean_data.outliers_to_col_avg(X, XTest)

# one-hot coding for "PRI_jet_num" (column 22)
(id, y, X) = clean_data.one_hot_PRI_jet_num(id, y, X)
(idTest, yTest, XTest) = clean_data.one_hot_PRI_jet_num(idTest, yTest, XTest)

################################################################################
#                      standardize & polynomial expansion                      #
################################################################################
X = polynomial_expansion.polynomial_expansion(X, degree)
(X, _, _) = helpers.standardize(X)
X = helpers.add_offset_parameter(X)

XTest = polynomial_expansion.polynomial_expansion(XTest, degree)
(XTest, _, _) = helpers.standardize(XTest)
XTest = helpers.add_offset_parameter(XTest)

################################################################################
#                                     train                                    #
################################################################################
N, D = X.shape
initial_w = np.random.uniform(initial_w_range[0], initial_w_range[1], D)

# # Train model & select "optimal" model parameters found
# (losses, ws) = gd_func(y, X, initial_w, max_iters, gamma, lamb)
# w_star = ws[-1]

# we provide the solution w_star so you don't have to wait 30 minutes for it.
w_star = np.array([-1.21813497e+00,   7.99064213e-01,   3.05112850e-02,  3.83370323e-01,   9.26110669e-02,  -3.84684047e-01, -1.61573278e-01,  -7.72953040e-03,   9.11551283e-01,  2.85754554e-02,   5.57842159e-02,  -1.84295771e-01,  1.03392233e-01,  -1.59186464e-01,   2.98753910e-01,  2.30552669e-02,  -2.60942065e-02,   3.08179904e-01,  3.02521641e-02,  -6.87848574e-03,   2.22788600e-01,  4.15724258e-03,  -6.54514432e-03,  -1.68735443e-01,  9.13926404e-03,  -1.11871409e-02,  -2.00330386e-01,  1.24493931e-03,  -1.38359462e-03,  -7.14964333e-02, -3.32197749e-02,  -2.87600932e-02,   1.69414797e-02,  7.90982880e-02,  -8.81033430e-01,  -3.81078748e-01, -7.44520801e-01,   3.16140225e-01,   2.23753933e-01,  1.54986756e-01,   9.94494823e-02,  -5.73505869e-01,  2.74788085e-02,  -8.29636103e-02,  -2.11594699e-02,  8.49854040e-02,   1.47830206e-01,  -3.18020650e-01, -1.19189686e-02,   2.79166227e-03,  -2.63083891e-01, -7.56918204e-04,   1.19705819e-02,  -1.88612960e-01,  8.30142451e-03,  -1.58715649e-03,   1.02629601e-01, -5.36204510e-03,   9.66173953e-03,   2.75718054e-01,  3.81892872e-03,   3.09059995e-02,   1.11535810e-01, -7.64442226e-02,   3.02749064e-01,   3.68936425e-01,  1.79082108e-01,   4.27388151e-01,  -3.62697301e-01, -3.69312548e-01,   8.13429792e-02,   6.99239215e-03, -2.73767792e-02,  -6.35845043e-01,   2.15979921e-01,  9.00134462e-02,   5.00068246e-02,   2.21557396e-01, -1.12179471e-02,   2.24664318e-01,  -2.89787677e-02, -1.40817057e-02,   3.13536919e-01,  -3.27959094e-03, -4.30547207e-03,   2.43535538e-01,   8.26137859e-03,  5.61392881e-03,  -9.07168841e-02,   1.11506079e-04, -1.01725523e-02,  -6.29353776e-02,   9.32081388e-03, -1.57639903e-02,  -1.20527613e-01,   1.32832478e-02, -3.74957633e-03,   2.97582844e-02,  -1.50852176e-03, -5.89277450e-01,   2.11222354e-01,   2.88622531e-02, -1.98672899e-02,   6.72650960e-02,  -6.75724686e-01,  3.13740150e-02,  -1.33134338e-01,  -2.00323802e-01,  1.05818936e-01,  -2.27317371e-02,  -2.54999628e-01,  1.02655905e-02,   4.02279350e-02,  -2.73337319e-01, -1.21088472e-02,  -2.38156603e-03,  -2.39258298e-01,  2.50514145e-03,  -6.38407778e-02,  -7.91216981e-02, -2.04215208e-02,   3.53440773e-03,  -2.77878673e-03,  6.24995389e-03,   4.76341709e-02,   5.64313801e-02, -1.92488575e-01,   1.70323369e-01,   3.25971568e-01,  1.93281734e-01,  -2.48497637e-02,  -3.07144317e-02,  4.44710145e-02,  -9.55404142e-03,   5.63262521e-01, -5.47687756e-03,   3.77805682e-02,   4.33761661e-02,  4.36105808e-01,  -1.07352235e-03,  -5.56496625e-02, -2.92366433e-03,  -3.25881370e-02,  -1.65093792e-01, -3.72421195e-02,   4.55480576e-02,  -8.51030173e-02, -6.04341873e-03,   5.96743913e-02,   1.95492078e-02, -8.42218831e-03,  -4.95115226e-02,   9.10044546e-02,  3.63198518e-02,  -3.17959422e-02,   1.12740709e-01, -9.81773020e-02,  -1.31373243e-01,   1.29348482e-01,  1.27792333e-01,  -1.94153866e-01,  -1.34622143e-02, -1.69325376e-02,   2.03318197e-01,  -4.49418484e-02, -7.69317271e-03,  -9.16482369e-02,  -1.12301171e-02,  2.76426626e-01,  -4.59248386e-02,  -2.31279405e-02,  8.85216588e-03,   8.53433306e-02,  -1.60954534e-02, -3.89491089e-02,   5.58907564e-02,  -1.11895755e-02, -1.22246358e-01,   8.90066171e-03,   1.10693593e-02,  1.04915927e-02,  -4.49930149e-02,   1.01029585e-02, -1.51436266e-02,  -2.13410041e-02,  -3.24256989e-02, -2.86759416e-02,  -1.83693271e-01,  -6.60498945e-02, -1.74082293e-02,  -1.14482263e-02,   1.70459738e-01, -2.36850293e-03,   2.55814550e-02,  -4.52425092e-03,  3.52416417e-02,   3.02774697e-01,   4.65747597e-02, -5.24642984e-03,   3.63801932e-02,   7.52705382e-02,  9.72054651e-03,   9.84714398e-03,   2.20108894e-02, -2.01168013e-02,  -1.54865856e-02,   4.22495911e-02,  4.27085325e-02,  -6.35566711e-03,  -6.05881612e-02, -4.09446799e-03,   5.75731099e-03,   5.94271644e-03, -3.26164624e-02,  -2.72787845e-02,  -2.50935746e-02, -1.32252162e-01,  -1.30037407e-01,   2.62383609e-02,  7.12479929e-03,  -2.13022538e-02,  -4.96215478e-02, -1.16774993e-02,  -1.25214126e-01,  -1.54556047e-02,  4.44434312e-03,   8.41215983e-03,   1.74678187e-02, -3.91074270e-04,  -2.25687070e-02,   7.46909426e-02,  9.02888199e-03,  -6.59347284e-03,  -4.95648736e-02,  2.05178921e-02,  -1.31266253e-02,  -1.37205252e-01, -1.80713453e-02,  -7.38152632e-03,  -3.36639490e-02,  3.32386999e-02,   2.79655663e-02,  -2.89548888e-02,  1.16190647e-02,  -1.01873899e-01,  -6.11101805e-02, -5.37731827e-02,  -2.37659441e-01,  -3.13073386e-01,  2.57843371e-01,   4.82533836e-01,  -1.45461997e-02, -1.01593863e-02,   8.42570334e-02,  -1.97236807e-02,  4.02901980e-02,  -5.31786807e-01,   3.12368758e-02,  8.57175668e-02,   4.61408149e-01,   2.34440289e-02,  7.44352833e-03,   2.54887395e-01,  -5.09697721e-03, -3.39821697e-02,  -2.31141777e-01,   4.24051307e-01,  1.29148789e-02,   1.09698017e-01,   2.85855021e-02,  7.75643489e-02,   4.92599386e-02,  -2.37009328e-02, -1.63900558e-01,  -1.61146371e-02,  -2.05536040e-03, -8.31617333e-03,  -9.11106202e-03,  -3.23216102e-02, -1.83941164e-02,  -4.88355474e-03,  -3.19212352e-04,  1.05053975e-02,  -9.83805832e-02,  -6.23862591e-02, -1.67487278e-03,  -1.41498598e-02,   3.28543526e-02,  1.43225627e-03,   7.30481959e-03,   6.98769379e-02, -9.62529804e-02,   9.39303540e-02,  -7.85643508e-03,  4.37512294e-02,   8.26480764e-03,   4.19068249e-02, -1.31632339e-01,   7.01590017e-03,  -5.40821867e-02, -8.13208584e-03,   6.76619178e-03,  -7.24494222e-02,  8.40125032e-03,  -1.53032401e-02,  -4.22280838e-02, -4.06863590e-03,   7.75692273e-04,   9.35219258e-03, -1.31450450e-02,   1.00284883e-02,   8.96718323e-03,  2.61769701e-03,  -6.02101004e-03,   3.93198933e-02,  1.90823161e-01,   7.62601580e-02,  -2.13039024e-02, -4.35704555e-02,   3.55986348e-02,   6.33828523e-02, -4.29939764e-02,   3.07596188e-01,  -2.63383852e-04,  8.73504643e-04,   1.40322746e-02,  -9.78464022e-03,  6.05769975e-03,  -1.88310793e-02,   3.82496682e-03,  3.83913391e-02,  -5.77138772e-02,   2.19615857e-03,  7.90056579e-03,  -8.80821844e-02,  -2.03735702e-02, -1.30519291e-02,   2.58484623e-02,  -1.48757528e-01, -7.18524488e-02,   4.82554974e-03,   2.45506842e-02, -2.57032990e-02,   7.38380744e-02,  -2.86347006e-01,  1.84865080e-03,   6.61871853e-03,  -1.99352313e-01, -1.36317767e-02,  -4.40218543e-03,   1.55728372e-01,  5.42521244e-03,   4.51203886e-02,  -2.88896745e-02,  4.95917389e-03,   8.66255739e-03,   1.04192751e-01,  2.41152429e-03,  -3.49994306e-03,  -1.08428489e-02,  6.31586462e-02,   5.10858265e-02,   6.03517025e-02,  4.71418012e-02,  -9.81403943e-02,   1.58435237e-02,  8.41478093e-03,  -9.67343154e-03,   8.02830606e-02, -1.48402758e-02,   7.07855968e-03,   3.73318812e-02, -2.38297922e-03,  -1.06048828e-01,  -4.84209522e-03, -3.02363065e-03,  -3.16767650e-02,   3.59641588e-02,  3.25637409e-03,   3.36228169e-04,  -1.64033462e-02, -3.34999604e-02,  -2.76025941e-02,  -4.76977822e-02, -5.24191429e-02,  -1.10525916e-01,   2.72482693e-02, -4.84168099e-03,  -9.73202659e-03,  -2.01740774e-02, -1.95712034e-02,   6.19987493e-02,   1.42032319e-03, -5.07864524e-02,   4.51353655e-02,  -2.61623042e-02,  2.07413172e-02,   1.39932387e-02,  -1.51665187e-02,  4.24649903e-03,  -2.60116190e-02,   2.66787464e-01,  3.49890105e-02,   5.12359405e-02,  -1.96828486e-02, -2.68234013e-01,  -9.79180403e-03,   1.48293825e-03,  2.90418187e-01,  -3.44771089e-03,   5.36629102e-03,  9.44081301e-03,   5.02041418e-03,  -5.67883983e-03, -4.22997003e-02,   1.72378312e-04,   7.92167382e-03,  4.53495397e-02,   1.94470273e-03,  -2.31712930e-02,  8.63737586e-03,   1.88416972e-02,   1.37254654e-02,  1.49197397e-03,  -3.03021652e-03,   2.11166410e-02,  1.26993085e-02,  -2.25976797e-02,   2.59720102e-02,  4.51150785e-02,  -1.29649687e-02,   1.32077035e-02,  1.08004805e-04,  -3.69446545e-03,  -7.91379689e-03,  7.20100915e-03,   4.92671813e-03,   3.65249460e-03, -1.78325604e-02,  -2.03703684e-02,  -5.30190216e-03, -1.51748153e-03,  -1.28832781e-01,   1.58299669e-02, -1.76324441e-02,   5.54596627e-02,  -2.62099551e-02, -1.14554210e-02,  -1.19366847e-02,  -1.60429194e-02,  8.02161330e-03,   9.88408418e-02,   1.68514804e-02,  1.50854797e-02,  -4.51827997e-02,   1.11544857e-01,  1.35275591e-01,   5.40040369e-02,  -1.75432929e-02, -3.19383067e-01,   1.09187025e-02,   7.45992011e-03, -8.90607816e-03,   5.25766471e-03,   1.99362452e-02, -3.05571615e-03,  -9.98107577e-04,  -6.14892939e-03,  7.96145232e-03,  -5.63237240e-03,   1.45733395e-02,  2.51773926e-02,   1.49216850e-02,   2.02162930e-03,  1.62060573e-02,  -2.34575178e-03,   3.03771359e-03,  1.36140133e-02,   2.32002296e-03,   1.84125596e-02, -8.47948323e-03,  -3.31352258e-02,  -1.69857515e-02, -1.48455191e-03,  -1.17216755e-02,  -8.55224124e-03, -8.40848450e-03,  -6.87263204e-03,   1.69690673e-03,  1.45413095e-03,  -1.42391666e-02,  -1.37272937e-02, -8.59833194e-02,  -9.00034816e-02,   5.66636173e-03,  2.41986078e-02,   6.52576012e-02,  -1.74639416e-02,  2.43882656e-02,  -8.08493892e-02,   1.87420265e-01,  7.49505808e-02,   7.20417693e-02,  -1.42265126e-02,  2.43628949e-03,  -1.84313378e-03,   5.09124991e-04, -4.01221622e-04,   2.11468978e-02,   2.92865115e-03, -5.60338342e-03,   2.19988389e-03,   2.72182064e-03, -1.40045568e-02,   1.10284853e-02,   1.20841233e-02,  7.49379714e-03,  -1.40448816e-02,  -2.24679841e-02,  3.47011619e-02,   5.16048831e-02,   5.89119144e-03, -9.20797218e-03,  -7.88548932e-03,   1.49192223e-02,  2.14393792e-01,   4.31059824e-02,  -2.05012295e-01,  4.89860911e-02,  -3.17372382e-02,   1.25887183e-02,  1.02021824e-03,  -3.88405474e-02,  -1.14409065e-02,  8.25324368e-03,   6.15589531e-03,  -3.24630029e-02,  5.01288765e-02,  -1.12180757e-01,  -7.48336311e-02,  3.57813748e-01,   3.09738153e-03,  -3.00165489e-02, -1.96892274e-02,  -6.63026620e-03,  -6.16628770e-03,  3.24687388e-02,  -3.66001378e-03,   1.59398191e-02,  4.65051845e-03,   1.01299540e-02,  -3.51035034e-02,  1.52764982e-02,   5.76623633e-03,   6.11417966e-03,  3.15704313e-02,  -6.83057630e-03,  -4.28346753e-03, -1.17045931e-02,  -1.18246783e-01,  -1.67412873e-03,  4.92261691e-03,  -4.41274760e-03,  -3.21693847e-02, -2.74392089e-02,   3.35931046e-02,  -1.09660753e-01,  2.52705139e-01,   2.77765221e-03,   1.91389879e-03,  3.25497546e-02,   2.85366822e-02,  -1.02954086e-05,  8.91734653e-04,  -3.61480432e-03,  -1.42486539e-02,  3.27415717e-02,   2.83215314e-02,  -4.36361344e-03,  2.04638731e-03,   6.34341119e-02,   4.53017769e-04,  4.98469992e-02,  -6.59353018e-02,  -5.19988600e-02, -3.28215812e-02,   1.50462194e-04,  -5.62069645e-04, -7.85663783e-04,  -2.79324395e-02,  -5.99052349e-04,  4.93552796e-04,   1.69177167e-02,   3.83051056e-04,  7.79895566e-02])

# Training accuracy
(correct_count, total_count) = helpers.prediction_accuracy(y, X, w_star)
correct_ratio = correct_count / total_count

################################################################################
#                            analysis of results                               #
################################################################################
print("classification precision: {cp}".format(cp=correct_ratio))

################################################################################
#                            store data for submission                         #
################################################################################
y_pred = proj1_helpers.predict_labels(w_star, XTest)
proj1_helpers.create_csv_submission(idTest, y_pred, DATA_PREDICTIONS_PATH)
